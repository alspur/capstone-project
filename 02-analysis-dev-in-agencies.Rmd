# Analysis development in education

## What does it look like in education agencies?

Now more than ever, the use of data is a critical component of strategic decision-making within organizations. As computing costs have dropped precipitously and the volume of raw data has grown exponentially, leaders in organizations are eager to leverage these trends to better inform their choices. 

The most significant barriers to a data-informed organization are no longer financial or technological. Instead, personnel and culture tend to be the largest obstacles for organizations looking to improve their use of data.

Nowhere is this more true than in education. For years, educators and policy makers have been encouraged to embrace "data-driven decision-making," but few education organizations have personnel that focus specifically on data analysis. Instead, it has become yet another skill we've asked educators to add to their already-overflowing professional responsibilities. 

Even in larger education organizations that are fortunate enough to have dedicated data analysts, such as state education agencies (SEA's), cultural barriers often prevent them from operating as effectively as they could. Analysts are often program-specific staff working on their own. Collaboration among data analysts in SEA's is frequently an exception, not the rule.

## Why should we care about the culture of data analysis?

Data analysis is difficult work. It requires an understanding of the field you're studying, statistical knowledge, some programming ability, and most importantly, the ability to translate your results for non-analysts.

At rstudio::conf() in January 2017, Hilary Parker gave a presentation that pinpoints some of the most critical challenges faced by data analysts. In her talk, she notes that "creating an analysis is a hard, error-ridden process that gets 'yadda-yadda-ed' away" because we're reluctant to share our own workflows and/or limit another analyst's "creativity." 

These norms between analyst can not only limit their own professional growth, it inevitably leads to mistakes that go unnoticed because there's only been one pair of eyes on a project. To address this problem, Parker recommends that we start to think about ourselves as "analysis developers" and borrow some concepts from programmers and engineers.

Errors will happen. Parker notes that we can learn from operations engineers, particularly Sidney Dekker's book, "Understanding Human Error." The lesson of this book is to not blame a person for their errors and instead to look at the process they used. This helps to make the after-action review conversations less personal and more focused on solving the problem at hand.

Reducing error is a big reason to care about developing a healthy, collaborative culture among "analysis developers," but there are many other benefits to improving how these folks work. It can help improve the quality and depth of analysis as they start to learn from one another. A more collaborative environment can also improve the speed of analysis as analysis development moves from an ad-hoc approach to one that follows a more standardized routine. This will also help to improve institutional knowledge of the analysis development process, making it easier to integrate new staff and maintain stability after a personnel transition. 

Education agencies could develop more accurate, high-quality, and reproducible analyses, but it order to do so, they need to deliberately build a strong collaborative culture among their analysis developers.
